{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 479: Deep Learning (Spring 2019)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat479-ss2019/\n",
    "GitHub repository: https://github.com/rasbt/stat479-deep-learning-ss19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 7.2.0\n",
      "\n",
      "torch 1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Runs on CPU or GPU (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet with Xavier (Uniform) initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "##########################\n",
    "### MNIST DATASET\n",
    "##########################\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # calculate same padding:\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # => p = (s(o-1) - w + k)/2\n",
    "        \n",
    "        # 28x28x1 => 28x28x4\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=1,\n",
    "                                      out_channels=4,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1) # (1(28-1) - 28 + 3) / 2 = 1\n",
    "        # 28x28x4 => 14x14x4\n",
    "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0) # (2(14-1) - 28 + 2) = 0                                       \n",
    "        # 14x14x4 => 14x14x8\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=4,\n",
    "                                      out_channels=8,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1) # (1(14-1) - 14 + 3) / 2 = 1                 \n",
    "        # 14x14x8 => 7x7x8                             \n",
    "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0) # (2(7-1) - 14 + 2) = 0\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(7*7*8, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_1(out)\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        \n",
    "        logits = self.linear_1(out.view(-1, 7*7*8))\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "    \n",
    "torch.manual_seed(random_seed)\n",
    "model = ConvNet(num_classes=num_classes)\n",
    "\n",
    "\n",
    "################################################\n",
    "########### REINITIALIZE WEIGHTS ###############\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.zero_(m.bias.detach())\n",
    "    \n",
    "model.apply(weights_init)\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/469 | Cost: 2.3226\n",
      "Epoch: 001/010 | Batch 050/469 | Cost: 1.8800\n",
      "Epoch: 001/010 | Batch 100/469 | Cost: 0.6941\n",
      "Epoch: 001/010 | Batch 150/469 | Cost: 0.6434\n",
      "Epoch: 001/010 | Batch 200/469 | Cost: 0.4129\n",
      "Epoch: 001/010 | Batch 250/469 | Cost: 0.3255\n",
      "Epoch: 001/010 | Batch 300/469 | Cost: 0.3067\n",
      "Epoch: 001/010 | Batch 350/469 | Cost: 0.1891\n",
      "Epoch: 001/010 | Batch 400/469 | Cost: 0.3091\n",
      "Epoch: 001/010 | Batch 450/469 | Cost: 0.4019\n",
      "Epoch: 001/010 training accuracy: 91.53%\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 002/010 | Batch 000/469 | Cost: 0.2708\n",
      "Epoch: 002/010 | Batch 050/469 | Cost: 0.2549\n",
      "Epoch: 002/010 | Batch 100/469 | Cost: 0.2998\n",
      "Epoch: 002/010 | Batch 150/469 | Cost: 0.2383\n",
      "Epoch: 002/010 | Batch 200/469 | Cost: 0.1199\n",
      "Epoch: 002/010 | Batch 250/469 | Cost: 0.2099\n",
      "Epoch: 002/010 | Batch 300/469 | Cost: 0.1645\n",
      "Epoch: 002/010 | Batch 350/469 | Cost: 0.1065\n",
      "Epoch: 002/010 | Batch 400/469 | Cost: 0.2079\n",
      "Epoch: 002/010 | Batch 450/469 | Cost: 0.2111\n",
      "Epoch: 002/010 training accuracy: 93.14%\n",
      "Time elapsed: 0.46 min\n",
      "Epoch: 003/010 | Batch 000/469 | Cost: 0.3230\n",
      "Epoch: 003/010 | Batch 050/469 | Cost: 0.1133\n",
      "Epoch: 003/010 | Batch 100/469 | Cost: 0.2814\n",
      "Epoch: 003/010 | Batch 150/469 | Cost: 0.2565\n",
      "Epoch: 003/010 | Batch 200/469 | Cost: 0.1692\n",
      "Epoch: 003/010 | Batch 250/469 | Cost: 0.1448\n",
      "Epoch: 003/010 | Batch 300/469 | Cost: 0.1377\n",
      "Epoch: 003/010 | Batch 350/469 | Cost: 0.1355\n",
      "Epoch: 003/010 | Batch 400/469 | Cost: 0.1383\n",
      "Epoch: 003/010 | Batch 450/469 | Cost: 0.1563\n",
      "Epoch: 003/010 training accuracy: 96.04%\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 004/010 | Batch 000/469 | Cost: 0.1172\n",
      "Epoch: 004/010 | Batch 050/469 | Cost: 0.1186\n",
      "Epoch: 004/010 | Batch 100/469 | Cost: 0.1309\n",
      "Epoch: 004/010 | Batch 150/469 | Cost: 0.1291\n",
      "Epoch: 004/010 | Batch 200/469 | Cost: 0.0515\n",
      "Epoch: 004/010 | Batch 250/469 | Cost: 0.1336\n",
      "Epoch: 004/010 | Batch 300/469 | Cost: 0.1862\n",
      "Epoch: 004/010 | Batch 350/469 | Cost: 0.1421\n",
      "Epoch: 004/010 | Batch 400/469 | Cost: 0.1289\n",
      "Epoch: 004/010 | Batch 450/469 | Cost: 0.0806\n",
      "Epoch: 004/010 training accuracy: 96.26%\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 005/010 | Batch 000/469 | Cost: 0.1097\n",
      "Epoch: 005/010 | Batch 050/469 | Cost: 0.1828\n",
      "Epoch: 005/010 | Batch 100/469 | Cost: 0.1365\n",
      "Epoch: 005/010 | Batch 150/469 | Cost: 0.1046\n",
      "Epoch: 005/010 | Batch 200/469 | Cost: 0.1373\n",
      "Epoch: 005/010 | Batch 250/469 | Cost: 0.0916\n",
      "Epoch: 005/010 | Batch 300/469 | Cost: 0.0834\n",
      "Epoch: 005/010 | Batch 350/469 | Cost: 0.1175\n",
      "Epoch: 005/010 | Batch 400/469 | Cost: 0.1268\n",
      "Epoch: 005/010 | Batch 450/469 | Cost: 0.0978\n",
      "Epoch: 005/010 training accuracy: 96.72%\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 006/010 | Batch 000/469 | Cost: 0.1857\n",
      "Epoch: 006/010 | Batch 050/469 | Cost: 0.1292\n",
      "Epoch: 006/010 | Batch 100/469 | Cost: 0.0978\n",
      "Epoch: 006/010 | Batch 150/469 | Cost: 0.0757\n",
      "Epoch: 006/010 | Batch 200/469 | Cost: 0.1328\n",
      "Epoch: 006/010 | Batch 250/469 | Cost: 0.0761\n",
      "Epoch: 006/010 | Batch 300/469 | Cost: 0.0729\n",
      "Epoch: 006/010 | Batch 350/469 | Cost: 0.1653\n",
      "Epoch: 006/010 | Batch 400/469 | Cost: 0.2019\n",
      "Epoch: 006/010 | Batch 450/469 | Cost: 0.0744\n",
      "Epoch: 006/010 training accuracy: 97.04%\n",
      "Time elapsed: 1.35 min\n",
      "Epoch: 007/010 | Batch 000/469 | Cost: 0.0593\n",
      "Epoch: 007/010 | Batch 050/469 | Cost: 0.0681\n",
      "Epoch: 007/010 | Batch 100/469 | Cost: 0.0654\n",
      "Epoch: 007/010 | Batch 150/469 | Cost: 0.1153\n",
      "Epoch: 007/010 | Batch 200/469 | Cost: 0.2162\n",
      "Epoch: 007/010 | Batch 250/469 | Cost: 0.0794\n",
      "Epoch: 007/010 | Batch 300/469 | Cost: 0.1189\n",
      "Epoch: 007/010 | Batch 350/469 | Cost: 0.1096\n",
      "Epoch: 007/010 | Batch 400/469 | Cost: 0.0360\n",
      "Epoch: 007/010 | Batch 450/469 | Cost: 0.0643\n",
      "Epoch: 007/010 training accuracy: 97.14%\n",
      "Time elapsed: 1.57 min\n",
      "Epoch: 008/010 | Batch 000/469 | Cost: 0.0793\n",
      "Epoch: 008/010 | Batch 050/469 | Cost: 0.0388\n",
      "Epoch: 008/010 | Batch 100/469 | Cost: 0.0218\n",
      "Epoch: 008/010 | Batch 150/469 | Cost: 0.1094\n",
      "Epoch: 008/010 | Batch 200/469 | Cost: 0.0901\n",
      "Epoch: 008/010 | Batch 250/469 | Cost: 0.0781\n",
      "Epoch: 008/010 | Batch 300/469 | Cost: 0.1152\n",
      "Epoch: 008/010 | Batch 350/469 | Cost: 0.1131\n",
      "Epoch: 008/010 | Batch 400/469 | Cost: 0.0582\n",
      "Epoch: 008/010 | Batch 450/469 | Cost: 0.1182\n",
      "Epoch: 008/010 training accuracy: 97.32%\n",
      "Time elapsed: 1.79 min\n",
      "Epoch: 009/010 | Batch 000/469 | Cost: 0.0841\n",
      "Epoch: 009/010 | Batch 050/469 | Cost: 0.0653\n",
      "Epoch: 009/010 | Batch 100/469 | Cost: 0.0852\n",
      "Epoch: 009/010 | Batch 150/469 | Cost: 0.0916\n",
      "Epoch: 009/010 | Batch 200/469 | Cost: 0.1298\n",
      "Epoch: 009/010 | Batch 250/469 | Cost: 0.0387\n",
      "Epoch: 009/010 | Batch 300/469 | Cost: 0.1307\n",
      "Epoch: 009/010 | Batch 350/469 | Cost: 0.1135\n",
      "Epoch: 009/010 | Batch 400/469 | Cost: 0.0915\n",
      "Epoch: 009/010 | Batch 450/469 | Cost: 0.0696\n",
      "Epoch: 009/010 training accuracy: 97.19%\n",
      "Time elapsed: 2.01 min\n",
      "Epoch: 010/010 | Batch 000/469 | Cost: 0.0441\n",
      "Epoch: 010/010 | Batch 050/469 | Cost: 0.1187\n",
      "Epoch: 010/010 | Batch 100/469 | Cost: 0.0571\n",
      "Epoch: 010/010 | Batch 150/469 | Cost: 0.0982\n",
      "Epoch: 010/010 | Batch 200/469 | Cost: 0.0784\n",
      "Epoch: 010/010 | Batch 250/469 | Cost: 0.1295\n",
      "Epoch: 010/010 | Batch 300/469 | Cost: 0.0673\n",
      "Epoch: 010/010 | Batch 350/469 | Cost: 0.1090\n",
      "Epoch: 010/010 | Batch 400/469 | Cost: 0.0619\n",
      "Epoch: 010/010 | Batch 450/469 | Cost: 0.1754\n",
      "Epoch: 010/010 training accuracy: 97.47%\n",
      "Time elapsed: 2.24 min\n",
      "Total Training Time: 2.24 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for features, targets in data_loader:\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "    \n",
    "    model = model.eval()\n",
    "    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "          epoch+1, num_epochs, \n",
    "          compute_accuracy(model, train_loader)))\n",
    "    \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.36%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy       1.15.4\n",
      "torch       1.0.1.post2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
